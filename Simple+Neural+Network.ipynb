{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building Simple Neural Network Using Numpy\n",
    "\n",
    "To gain better understanding and intuition about Neural Network is to build a Nueral Net from Scratch with out using high end libraries like Tensorflow, Keras, theano etc.The reason for it is because it abstract or hides the inner working of maths involved in the Neural net.\n",
    "so lets talk about what Neural Net is. Most introductory info describes the working of Neural Net as brain. like every neuron in the human brain is connected with other neurons for communication and performing different tasks of the human body. Similary the Neural Network nodes is also connected to one an other for effective communication and producing better outputs based on the inputs. In mathematics we can describe Neural Network as a mathematical function that maps a given input to a desired output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Networks consist of the following components\n",
    "An input layer, X\n",
    "\n",
    "An arbitrary amount of hidden layers\n",
    "\n",
    "An output layer, Y\n",
    "\n",
    "A set of weights and biases between each layer, W and b\n",
    "\n",
    "A choice of activation function for each hidden layer, σ. In this tutorial, we’ll use a Sigmoid activation function.\n",
    "\n",
    "\n",
    "Below is a diagram of two layer architecture.\n",
    "\n",
    "![title](Desktop\\img\\2layer.jpg)\n",
    "\n",
    "                                                  Figure no #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy is the core library for scientific computing in Python. It provides a high-performance multidimensional array object, and tools for working with these arrays. It has very useful mathematical methods which will help us in completing this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural networks work in very similar manner. It takes several input in our case 2 inputs, processes it through multiple neurons from multiple hidden layers and returns the result using an output layer. This result estimation process is technically known as FORWARD PROPAGATION.\n",
    "\n",
    "Next, we compare the result with actual output. The task is to make the output to neural network as close to actual (desired) output. Each of these neurons are contributing some error to final output. How do you reduce the error?\n",
    "\n",
    "We try to minimize the value/ weight of neurons those are contributing more to the error and this happens while traveling back to the neurons of the neural network and finding where the error lies. This process is known as BACK PROPAGATION. In order to minimize the error, the neural networks use a common algorithm known as “Gradient Descent”.\n",
    "\n",
    "![title](Desktop\\img\\feed.jpg)\n",
    "\n",
    "                                             Figure no #2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So lets start coding by making the class of our Neural Network. \n",
    "So the first method is to build the constructor. Here we will initialize the inputLayerSize, hiddenLayerSize, outputLayerSize and the weights W1 and W2 and making baises to 0.\n",
    "\n",
    "According to above diagram we have two inputs, one hidden layer three activations and one output layer.\n",
    "\n",
    "Forward Propagation:\n",
    "\n",
    "Step 1: We have to model the relationship between two variables by fitting a linear equation to observed data.\n",
    "which is Y= ax + b. here we have weight (W) instead (a) so the equation becomes y=Wx +b. In this step we take matrix dot product of input and weights assigned to edges between the input and hidden layer then add biases of the hidden layer neurons to respective inputs, this is known as linear transformation.\n",
    "\n",
    "Step 2: Perform non-linear transformation using an activation function (Sigmoid) on the data obtained from previous linear equation task restrict and classify the values between 0 and 1 and also used as output activation function for binary classification problems. Sigmoid will return the output as 1/(1 + exp(-x)).\n",
    "\n",
    "Step 3: Perform a linear transformation on hidden layer activation (take matrix dot product with weights and add a bias of the output layer neuron) then apply an activation function (again used sigmoid, but you can use any other activation function depending upon your task) to predict the output.\n",
    "\n",
    "![title](Desktop\\img\\sig.jpg)\n",
    "\n",
    "                                                    Figure no #3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputLayerSize=2  #intilialize Input, Hidden and Output layers\n",
    "        self.outputLayerSize=1\n",
    "        self.hiddenLayerSize=3\n",
    "    \n",
    "     \n",
    "        self.W1=np.random.randn(self.inputLayerSize,self.hiddenLayerSize) \n",
    "        self.W2=np.random.randn(self.hiddenLayerSize,self.outputLayerSize)\n",
    "        #initialize waits using np.random.randn() which creates an array of specified shape and fills it with random values\n",
    "        #as per standard normal distribution.\n",
    "        \n",
    "    \n",
    "    def sigmoid(self,z):         #Sigmoid Function\n",
    "        return 1/(1+np.exp(-z)) \n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z1=np.dot(X,self.W1)    # will do matrix multiplication eg:Matrix \n",
    "                                         #a:\n",
    "                                         #[[1 2]\n",
    "                                        # [3 4]]\n",
    "                                        #Matrix b:\n",
    "                                         #[[ 5 10]\n",
    "                                         # [15 20]]\n",
    "                                        #Resultant Matrix:\n",
    "                                       # [[ 35  50]\n",
    "                                         #[ 75 110]]\n",
    "\n",
    "        self.a1=self.sigmoid(self.z1)  #restricting values values between 0 and 1\n",
    "        self.z2=np.dot(self\n",
    "                       .a1,self.W2) #again matrix multiplication using prev values of a2 and W2\n",
    "        yHat=self.sigmoid(self.z2)  # sigmoid on z3 which have values from a2 and W2\n",
    "        return yHat\n",
    "     \n",
    "    def costFunction(self, X,y):   #calculate error between actual and predicted values\n",
    "        self.yHat=self.forward(X)\n",
    "        J=0.5*sum((y-self.yHat)**2)\n",
    "        return J\n",
    "        \n",
    "    def sigmoidPrime(self,z):\n",
    "        return np.exp(-z)/((1+np.exp(-z))**2) #derivative of Sigmoid funtion\n",
    "    \n",
    "    def costFunctionPrime(self,X,y):   #Applying chain rule and partial derivative to get the derivatives of each layer in NN\n",
    "        self.yHat=self.forward(X)\n",
    "        delta3=np.multiply(-(y-self.yHat),self.sigmoidPrime(self.z2))  #Using Equation 5 and Equation 6 in Figure no #5 \n",
    "        djdW2=np.dot(self.a1.T,delta3)                                 #for gradient of output layers weights\n",
    "        \n",
    "        delta2=np.dot(delta3,self.W2.T)*self.sigmoidPrime(self.z1)   # Using Equation 11 in Figure no #7 for finding \n",
    "        djdW1=np.dot(X.T,delta2)                                     #the gradient of hidden layer weights\n",
    "        \n",
    "        return djdW1,djdW2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: After Forward Propogation we have to Compare prediction with actual output and calculate the gradient of error (Actual – Predicted). Error is the mean square loss = ((Y-yHat)^2)/2\n",
    "\n",
    "Step 5: Now that we’ve measured the error of our prediction (loss), we need to find a way to propagate the error back, and to update our weights and biases.\n",
    "In order to know the appropriate amount to adjust the weights and biases by, we need to know the derivative of the loss function with respect to the weights and biases in our case biases are zero.\n",
    "\n",
    "![title](Desktop\\img\\Back.jpg)\n",
    "\n",
    "                                                    Figure no #4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients for Output Layer Weights:\n",
    "\n",
    "Output layer connection weights, wjk\n",
    "\n",
    "Since the output layer parameters directly affect the value of the error function, determining the gradients for those parameters is fairly straight-forward.\n",
    "\n",
    "\n",
    "![title](Desktop\\img\\der111.jpg)\n",
    "\n",
    "                                                Figure no #5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients for Hidden Layer Weights\n",
    "\n",
    "Due to the indirect affect of the hidden layer on the output error, calculating the gradients for the hidden layer weights (wij)  is somewhat more involved. However, the process starts just the same:\n",
    "\n",
    "![title](Desktop\\img\\der222.jpg)\n",
    "\n",
    "                                                 Figure no #6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](Desktop\\img\\back1.jpg)\n",
    "\n",
    "                                                   Figure no #7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to get better understaning of the derivation i put the link where there is detailed info about the derivatives of each layer step by step\n",
    "https://theclevermachine.wordpress.com/2014/09/06/derivation-error-backpropagation-gradient-descent-for-neural-networks/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X=np.array([[0,0],[1,0],[0,1],[1,1]])            #input array of X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=np.array([0,1,1,1])  #output actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 2)"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here i will train model for OR gate operation where A HIGH output (1) results if one or both the inputs to the gate are HIGH (1). If neither input is high, a LOW output (0) results.\n",
    "\n",
    "here i will get random values for X between 0 and 1\n",
    "where y is the addition of operand or element of X "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(4)\n",
    "X=np.random.rand(10000,2)    #getting random 10000 samples using np.random.rand() which output random values between 0 and 1\n",
    "y=np.apply_along_axis(lambda element: element[0]+element[1],axis=1,arr=X)\n",
    "#apply_along_axis will apply the lambda fucntion which is element wise addition of values using array X along column axis\n",
    "#y=np.random.rand(10000,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 2)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96702984,  0.54723225],\n",
       "       [ 0.97268436,  0.71481599],\n",
       "       [ 0.69772882,  0.2160895 ],\n",
       "       ..., \n",
       "       [ 0.63652491,  0.84404444],\n",
       "       [ 0.01889563,  0.3513995 ],\n",
       "       [ 0.0708804 ,  0.09866774]])"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.51426209,  1.68750035,  0.91381832, ...,  1.48056935,\n",
       "        0.37029513,  0.16954813])"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y=y.reshape(10000,1) #rehshape the array in to vector array "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.51426209],\n",
       "       [ 1.68750035],\n",
       "       [ 0.91381832],\n",
       "       ..., \n",
       "       [ 1.48056935],\n",
       "       [ 0.37029513],\n",
       "       [ 0.16954813]])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X=np.linalg.norm(X)\n",
    "#y=np.linalg.norm(y)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 955.62804742]\n",
      "[ 470.59807765]\n",
      "[ 458.17174786]\n",
      "[ 456.84621598]\n",
      "[ 453.21727266]\n",
      "[ 463.60520336]\n",
      "[ 473.39717028]\n",
      "[ 529.75791616]\n",
      "[ 581.31544319]\n",
      "[ 448.99298218]\n"
     ]
    }
   ],
   "source": [
    "NN=Neural_Network()       #creating an object of Neural Network class\n",
    "max_iteration=10000        #no of iteration\n",
    "iter=0\n",
    "learningRate=0.01        #learning rate its like taking steps toward local optima.  it should not be maximum nor minimum \n",
    "while iter<max_iteration:\n",
    "    djdW1, djdW2 = NN.costFunctionPrime(X,y)    #return the trained parameters and assign to djdW1, djdW2\n",
    "    \n",
    "    NN.W1=NN.W1-learningRate*djdW1    #update Parameter W1 and W2 based on djdW1 and djdW2\n",
    "    NN.W2=NN.W2-learningRate*djdW2\n",
    "    \n",
    "    if iter%1000==0:\n",
    "        print (NN.costFunction(X,y))  #print the costfucntion after every 1000 iteration\n",
    "    iter=iter+1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.51426209]\n",
      " [ 1.68750035]\n",
      " [ 0.91381832]\n",
      " ..., \n",
      " [ 1.48056935]\n",
      " [ 0.37029513]\n",
      " [ 0.16954813]]\n",
      "---\n",
      "[[ 0.99490012]\n",
      " [ 0.99693468]\n",
      " [ 0.94275001]\n",
      " ..., \n",
      " [ 0.99545428]\n",
      " [ 0.25156713]\n",
      " [ 0.05233312]]\n"
     ]
    }
   ],
   "source": [
    "print (y)\n",
    "print(\"---\")\n",
    "print (NN.forward(X))  # here we just check how our model perform by comparing actual values and the predicted ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "sse is same function as we used for cost function it is the sum of the squared differences between each actual and predicted observations. here i just used to see if the model is working well and how much error we have in our predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08943408]\n"
     ]
    }
   ],
   "source": [
    "def sse(y,yHat):\n",
    "    return sum(map(lambda ab : pow(ab[0]-ab[1],2), zip(y,yHat)))\n",
    "#print(sse(y,NN.forward(X)))\n",
    "print(sse(y,NN.forward(X))/10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.34941852  0.30649525]]\n",
      "[[ 0.77974212]]\n"
     ]
    }
   ],
   "source": [
    "pred=np.random.rand(1,2)\n",
    "print(pred)               #lets take random sample check the difference between the actual and predicted value by the model\n",
    "                            #the predicted result is quite close to the actual value if u run this code snippet\n",
    "result=NN.forward(pred)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets create an other object Neural Network Class and use the Sum of squared error method. this time we didnt train the model and the SSE will produce greater error than the output we get from trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NN=Neural_Network() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.36054183]\n"
     ]
    }
   ],
   "source": [
    "#print(sse(y,NN.forward(X)))\n",
    "\n",
    "print(sse(y,NN.forward(X))/10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
